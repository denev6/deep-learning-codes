{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Denev6/practice/blob/main/transformer/t5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qXlXpiWjwG8"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-8t-yvMzXXf"
      },
      "source": [
        "Make sure to restart runtime after installing `sentencepiece`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kt9LGLSXyZpq",
        "outputId": "9cdb096f-051a-4413-e51c-abf3e10c6bb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.7.15\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SskIfXoWvyS_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import Trainer, TrainingArguments, T5Tokenizer, T5ForConditionalGeneration\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.model_selection import train_test_split "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLO55TxdfYuV",
        "outputId": "293e2281-d019-48fd-ac64-df0a9afdd952"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount(\"/content/drive\")\n",
        "CWD = \"/content/drive/MyDrive/T5\"\n",
        "\n",
        "def join_path(*args):\n",
        "    return os.path.join(CWD, *args)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "TRAIN_CSV = join_path(\"data\", \"train.csv\")\n",
        "TEST_CSV = join_path(\"data\", \"test.csv\")\n",
        "MODEL = \"t5-base\"\n",
        "MODEL_DIR = \"t5-base\"\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 30\n",
        "MAX_LENGTH = 128\n",
        "\n",
        "TRAIN_ARGS = TrainingArguments(\n",
        "    output_dir=join_path(MODEL_DIR),\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,  \n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    learning_rate=7e-6,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=1e-5,\n",
        "    dataloader_num_workers=0,\n",
        "    save_total_limit=1,\n",
        "    save_strategy=\"no\",\n",
        "    evaluation_strategy=\"epoch\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFZRDzdWkoTz"
      },
      "outputs": [],
      "source": [
        "train_csv = pd.read_csv(TRAIN_CSV)\n",
        "train_csv.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDYxMFkv7E9d",
        "outputId": "e975493e-a214-4963-f8a6-2ccac4ed2221"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "targets = train_csv[\"Target\"].unique()\n",
        "target_size = targets.shape[0]\n",
        "target_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCp43NZrqBJ7"
      },
      "outputs": [],
      "source": [
        "train_csv = train_csv.loc[:, [\"Utterance\", \"Target\"]]\n",
        "\n",
        "df_train, df_eval = train_test_split(train_csv, test_size=0.2)\n",
        "df_train.reset_index(drop=True, inplace=True)\n",
        "df_eval.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2RDMQSwv_QR"
      },
      "outputs": [],
      "source": [
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, target_size=7, mode=None, max_len=512):\n",
        "        self.data_col = \"Utterance\"\n",
        "        self.target_col = \"Target\"\n",
        "        self.mode = str(mode).strip().lower()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.target_size = target_size\n",
        "        self.max_len = max_len\n",
        "        self.data = self._tokenize(df)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        data, target = self.data[index]\n",
        "        data_ids = data[\"input_ids\"].squeeze()\n",
        "        data_mask = data[\"attention_mask\"].squeeze()\n",
        "\n",
        "        if self.mode != \"test\":\n",
        "            target_mask = target[\"attention_mask\"].squeeze()\n",
        "            labels = target[\"input_ids\"].squeeze()\n",
        "            labels[labels[:] == self.tokenizer.pad_token_id] = -100\n",
        "            return {\n",
        "                \"input_ids\": data_ids, \n",
        "                \"attention_mask\": data_mask, \n",
        "                \"decoder_attention_mask\": target_mask, \n",
        "                \"labels\": labels\n",
        "            }\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": data_ids, \n",
        "            \"attention_mask\": data_mask\n",
        "        }\n",
        "\n",
        "    def _tokenize(self, df):\n",
        "        res = list()\n",
        "        for index in range(df.shape[0]):\n",
        "            if self.mode == \"test\":\n",
        "                tokenized_target = None\n",
        "            else:\n",
        "                target = df.loc[index, self.target_col]\n",
        "                tokenized_target = self.tokenizer(\n",
        "                    [target],\n",
        "                    max_length=self.target_size, \n",
        "                    padding=\"max_length\", \n",
        "                    truncation=True, \n",
        "                    return_tensors=\"pt\"\n",
        "                )\n",
        "            \n",
        "            data = df.loc[index, self.data_col]\n",
        "            tokenized_data = self.tokenizer(\n",
        "                [data], \n",
        "                max_length=self.max_len, \n",
        "                padding=\"max_length\", \n",
        "                truncation=True, \n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "            res.append([tokenized_data, tokenized_target])\n",
        "        return res "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i04WXK3Yk9oi"
      },
      "outputs": [],
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained(\n",
        "    MODEL,\n",
        "    max_length=MAX_LENGTH\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zxN5atGQDpz"
      },
      "outputs": [],
      "source": [
        "train_set = EmotionDataset(\n",
        "    df_train, \n",
        "    tokenizer, \n",
        "    target_size=target_size, \n",
        "    mode=\"train\", \n",
        "    max_len=MAX_LENGTH\n",
        ")\n",
        "eval_set = EmotionDataset(\n",
        "    df_eval, \n",
        "    tokenizer, \n",
        "    target_size=target_size, \n",
        "    mode=\"train\", \n",
        "    max_len=MAX_LENGTH\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GujOmIO-0aV9"
      },
      "outputs": [],
      "source": [
        "model = T5ForConditionalGeneration.from_pretrained(\n",
        "    MODEL,\n",
        "    num_labels=target_size,\n",
        "    ignore_mismatched_sizes=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JngfHikz3Zxo"
      },
      "outputs": [],
      "source": [
        "def eval(model, tokenizer, eval_set):\n",
        "    model.to(DEVICE)\n",
        "    pred_values = list()\n",
        "    \n",
        "    for data in eval_set:\n",
        "        output = model.generate(\n",
        "            input_ids=data[\"input_ids\"].unsqueeze(0).to(DEVICE), \n",
        "            attention_mask=data[\"attention_mask\"].unsqueeze(0).to(DEVICE), \n",
        "            max_length=512\n",
        "        )\n",
        "        pred = [tokenizer.decode(ids) for ids in output][0]\n",
        "        re_tag = re.compile(\"<.*?>\")\n",
        "        pred = re.sub(re_tag, \"\", pred).strip()\n",
        "        pred_values.append(pred)\n",
        "\n",
        "    real_values = df_eval[\"Target\"].tolist()\n",
        "    acc = accuracy_score(real_values, pred_values)\n",
        "    f1 = f1_score(real_values, pred_values, average=\"macro\")\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "        \"f1\": f1\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uLtQqBDq_Rr"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=TRAIN_ARGS,\n",
        "    train_dataset=train_set,\n",
        "    eval_dataset=eval_set\n",
        ")\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6z0IrePhOh8V"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "up7mvzihZwR9"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(join_path(MODEL_DIR))\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pt6xKmtEYs6y"
      },
      "outputs": [],
      "source": [
        "score = eval(model, tokenizer, eval_set)\n",
        "print(f\"Accuracy: {score['accuracy']:.5f}\")\n",
        "print(f\"F1-macro: {score['f1']:.5f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzFz1q2DeJP0"
      },
      "outputs": [],
      "source": [
        "def classify(model, test_set):\n",
        "    model.to(DEVICE)\n",
        "    pred_values = list()\n",
        "    \n",
        "    for data in test_set:\n",
        "        output = model.generate(\n",
        "            input_ids=data[\"input_ids\"].unsqueeze(0).to(DEVICE), \n",
        "            attention_mask=data[\"attention_mask\"].unsqueeze(0).to(DEVICE), \n",
        "            max_length=512\n",
        "        )\n",
        "        pred = [tokenizer.decode(ids) for ids in output][0]\n",
        "        re_tag = re.compile(\"<.*?>\")\n",
        "        pred = re.sub(re_tag, \"\", pred).strip()\n",
        "        pred_values.append(pred)\n",
        "    \n",
        "    return pred_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeuRALPEvepl"
      },
      "outputs": [],
      "source": [
        "test_csv = pd.read_csv(join_path(\"data\", \"test.csv\"))\n",
        "df_test = test_csv.loc[:, \"Utterance\"].to_frame()\n",
        "test_set = EmotionDataset(df_test, tokenizer)\n",
        "\n",
        "preds = classify(model, test_set)\n",
        "test_csv[\"Target\"] = preds\n",
        "\n",
        "result_csv = test_csv.loc[:, [\"ID\", \"Target\"]]\n",
        "result_csv.head()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
